wandb_version: 1

_wandb:
  desc: null
  value:
    cli_version: 0.12.7
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    python_version: 3.7.12
    start_time: 1637813807
    t:
      1:
      - 1
      2:
      - 1
      3:
      - 16
      4: 3.7.12
      5: 0.12.7
      8:
      - 6
action_optimizer:
  desc: null
  value:
    _target_: mbrl.planning.CEMOptimizer
    alpha: ${overrides.cem_alpha}
    clipped_normal: ${overrides.cem_clipped_normal}
    device: ${device}
    elite_ratio: ${overrides.cem_elite_ratio}
    lower_bound: ???
    num_iterations: ${overrides.cem_num_iters}
    population_size: ${overrides.cem_population_size}
    return_mean_elites: true
    upper_bound: ???
algorithm:
  desc: null
  value:
    agent:
      _target_: mbrl.third_party.pytorch_sac.agent.sac.SACAgent
      action_dim: ???
      action_range: ???
      actor_betas:
      - 0.9
      - 0.999
      actor_cfg: ${algorithm.diag_gaussian_actor}
      actor_lr: ${overrides.sac_actor_lr}
      actor_update_frequency: ${overrides.sac_actor_update_frequency}
      alpha_betas:
      - 0.9
      - 0.999
      alpha_lr: ${overrides.sac_alpha_lr}
      batch_size: 256
      critic_betas:
      - 0.9
      - 0.999
      critic_cfg: ${algorithm.double_q_critic}
      critic_lr: ${overrides.sac_critic_lr}
      critic_target_update_frequency: ${overrides.sac_critic_target_update_frequency}
      critic_tau: 0.005
      device: ${device}
      discount: 0.99
      init_temperature: 0.1
      learnable_temperature: true
      obs_dim: ???
      target_entropy: ${overrides.sac_target_entropy}
    backwards_agent:
      _target_: mbrl.third_party.pytorch_sac.agent.sac.BackwardsSACAgent
      action_dim: ???
      action_range: ???
      actor_betas:
      - 0.9
      - 0.999
      actor_cfg: ${algorithm.diag_gaussian_actor}
      actor_lr: ${overrides.sac_actor_lr}
      actor_update_frequency: ${overrides.sac_actor_update_frequency}
      alpha_betas:
      - 0.9
      - 0.999
      alpha_lr: ${overrides.sac_alpha_lr}
      batch_size: 256
      critic_betas:
      - 0.9
      - 0.999
      critic_cfg: ${algorithm.double_q_critic}
      critic_lr: ${overrides.sac_critic_lr}
      critic_target_update_frequency: ${overrides.sac_critic_target_update_frequency}
      critic_tau: 0.005
      device: ${device}
      discount: 0.99
      init_temperature: 0.1
      learnable_temperature: true
      obs_dim: ???
      target_entropy: ${overrides.sac_target_entropy}
    diag_gaussian_actor:
      _target_: mbrl.third_party.pytorch_sac.agent.actor.DiagGaussianActor
      action_dim: ${algorithm.agent.action_dim}
      hidden_depth: ${overrides.sac_hidden_depth}
      hidden_dim: 1024
      log_std_bounds:
      - -5
      - 2
      obs_dim: ${algorithm.agent.obs_dim}
    double_q_critic:
      _target_: mbrl.third_party.pytorch_sac.agent.critic.DoubleQCritic
      action_dim: ${algorithm.agent.action_dim}
      hidden_depth: ${overrides.sac_hidden_depth}
      hidden_dim: 1024
      obs_dim: ${algorithm.agent.obs_dim}
    freq_train_model: ${overrides.freq_train_model}
    initial_exploration_steps: 5000
    learned_rewards: true
    name: mbpo_backwards
    normalize: true
    normalize_double_precision: true
    num_eval_episodes: 1
    random_initial_explore: false
    sac_samples_action: true
    target_is_delta: true
debug_mode:
  desc: null
  value: true
device:
  desc: null
  value: cuda:0
dynamics_model:
  desc: null
  value:
    _target_: mbrl.models.GaussianMLP
    activation_fn_cfg:
      _target_: torch.nn.SiLU
    deterministic: false
    device: ${device}
    ensemble_size: 7
    hid_size: 200
    in_size: ???
    learn_logvar_bounds: false
    num_layers: 4
    out_size: ???
    propagation_method: random_model
experiment:
  desc: null
  value: default
log_frequency_agent:
  desc: null
  value: 1000
overrides:
  desc: null
  value:
    effective_model_rollouts_per_step: 400
    env: cartpole_continuous
    epoch_length: 200
    freq_train_model: 200
    model_batch_size: 256
    model_lr: 0.001
    model_wd: 5.0e-05
    num_elites: 5
    num_epochs_to_retain_sac_buffer: 1
    num_sac_updates_per_step: 20
    num_steps: 5000
    patience: 5
    rollout_schedule:
    - 1
    - 15
    - 1
    - 1
    sac_actor_lr: 0.0003
    sac_actor_update_frequency: 1
    sac_alpha_lr: 0.0001
    sac_critic_lr: 0.0003
    sac_critic_target_update_frequency: 4
    sac_hidden_depth: 2
    sac_target_entropy: -0.05
    sac_updates_every_steps: 1
    trial_length: 200
    validation_ratio: 0.2
root_dir:
  desc: null
  value: ./exp
save_video:
  desc: null
  value: false
seed:
  desc: null
  value: 0
